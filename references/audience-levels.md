# Audience-Level Explanation Templates

## ğŸ‘¶ High School Student

**Frame:** The machine looks at data, finds patterns, then guesses answers.

**Template:**
- Imagine a row of numbers (like a barcode)
- The algorithm processes that row in its own way (bending / comparing / splitting / counting)
- At the end, it picks an answer
- If wrong, it adjusts (or not â€” some algorithms don't learn at all!)

**Vocabulary:** numbers, list, bending, comparing, closest, splitting, pattern, answer, adjusting

**Avoid:** matrix, gradient, loss function, manifold, representation, hyperplane

**Analogies by algorithm:**
- DL: "báº» cong dÃ£y sá»‘ nhiá»u láº§n rá»“i chá»n Ä‘Ã¡p Ã¡n"
- KNN: "há»i 3 ngÆ°á»i Ä‘á»©ng gáº§n nháº¥t, theo sá»‘ Ä‘Ã´ng"
- Decision Tree: "chÆ¡i 20 cÃ¢u há»i â€” há»i yes/no liÃªn tá»¥c"
- Naive Bayes: "Ä‘áº¿m xem thÆ°á»ng gáº·p nháº¥t lÃ  gÃ¬ rá»“i Ä‘oÃ¡n"
- K-Means: "chia báº¡n bÃ¨ thÃ nh nhÃ³m theo chá»— Ä‘á»©ng"
- SVM: "káº» 1 Ä‘Æ°á»ng chia 2 phe, cÃ¡ch xa nháº¥t cÃ³ thá»ƒ"

---

## ğŸ‘¨â€ğŸ’» Developer

**Frame:** An algorithm is a function: `prediction = f(data, parameters)`

**Template:**
- Input: array/matrix of shape `(n_samples, n_features)`
- Operation: [specific to algorithm â€” matmul / distance / split / count]
- Output: prediction (class label, probability, continuous value, or cluster)
- Learning: how parameters update (gradient, split criterion, frequency count, or nothing)
- Complexity: time/space for train and predict

**Vocabulary:** array, shape, function, parameter, fit, predict, complexity

**Code-first:** always show the sklearn/torch equivalent and explain params.

---

## ğŸ‘¨â€ğŸ”¬ ML Engineer

**Frame:** We're shaping decision boundaries in feature space via optimization.

**Template:**
- Input space: data manifold in R^n
- Hypothesis class: what boundaries can this model represent?
- Optimization: how does it search the hypothesis space?
- Inductive bias: what assumptions does this algorithm make?
- Generalization: bias-variance tradeoff and regularization

**Vocabulary:** hypothesis space, inductive bias, generalization bound, VC dimension, bias-variance, regularization

**Depth markers:**
- Discuss computational complexity (train vs predict)
- Compare sample efficiency across algorithms
- Mention when the algorithm breaks down (edge cases)
- Discuss relationship to other algorithms (e.g., Logistic Reg = 1-layer neural net)

---

## Switching Between Levels

| Signal | Level |
|---|---|
| "What is AI / ML?" | High school |
| "How do I implement this?" / "What params should I tune?" | Developer |
| "What's the inductive bias?" / "Bias-variance tradeoff?" | ML Engineer |

When unsure, start at developer level â€” safest middle ground.
